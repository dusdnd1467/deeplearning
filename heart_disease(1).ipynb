{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMX0gbrxU9vfmC3CD3Ql+KC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dusdnd1467/deeplearning/blob/master/heart_disease(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xe_uy9OXUmq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# XPT 파일 불러오기\n",
        "df = pd.read_sas(\"/content/LLCP2023.XPT\", format=\"xport\")\n",
        "\n",
        "# CSV 파일로 저장\n",
        "df.to_csv(\"2023 BRFSS.csv\", index=False)\n",
        "\n",
        "df = pd.read_csv(\"/content/2023 BRFSS.csv\")\n",
        "\n",
        "#변수 추출\n",
        "df_preporcessing = df[['_MICHD', '_RFHYPE6', 'TOLDHI3', 'CHOLCHK3', '_BMI5', 'SMOKE100', 'CVDSTRK3', 'DIABETE4', '_TOTINDA', 'DRNKANY6', '_RFBING6', '_HLTHPL1', 'MEDCOST1', 'GENHLTH', 'MENTHLTH', 'PHYSHLTH', 'DIFFWALK', '_SEX', '_AGEG5YR', 'EDUCA', 'INCOME3']]\n",
        "\n",
        "df_preporcessing.to_csv(\"preporcessing.csv\", index=False)\n",
        "\n",
        "# 1) 원본에서 88 → 0으로 치환\n",
        "df_preporcessing.loc[:, ['MENTHLTH', 'PHYSHLTH']] = (df_preporcessing.loc[:, ['MENTHLTH', 'PHYSHLTH']].replace(88, 0))\n",
        "\n",
        "# 2) 데이터 구분\n",
        "df_delA = df_preporcessing[['_RFHYPE6', 'TOLDHI3', 'CHOLCHK3', 'SMOKE100', 'CVDSTRK3',\n",
        "                            'DIABETE4', '_TOTINDA', 'DRNKANY6', '_RFBING6', '_HLTHPL1',\n",
        "                            'MEDCOST1', 'GENHLTH', 'DIFFWALK', 'EDUCA']]  # 7, 8\n",
        "df_delB = df_preporcessing[['MENTHLTH', 'PHYSHLTH', 'INCOME3']]  # 77, 99\n",
        "df_delC = df_preporcessing['_AGEG5YR']  # 14\n",
        "\n",
        "# 3) 이상치 제거(답변 거부, 공란 등)\n",
        "df_cleaned = df_preporcessing[~df_preporcessing[df_delA.columns].isin([7, 9, 77, 99]).any(axis=1)]\n",
        "df_cleaned = df_cleaned[~df_cleaned[df_delB.columns].isin([77, 99]).any(axis=1)]\n",
        "df_cleaned = df_cleaned[~df_cleaned['_AGEG5YR'].isin([14])]\n",
        "\n",
        "# 4) 결측치 제거\n",
        "df_cleaned = df_cleaned.dropna(axis=0, subset=None, inplace=False)\n",
        "\n",
        "# 5) CSV로 저장\n",
        "df_cleaned.to_csv(\"heart_disease_BRFSS2023.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------- 설정 --------------------\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "original_csv = \"/content/heart_disease_BRFSS2023.csv\"\n",
        "target_col   = \"_MICHD\"\n",
        "\n",
        "# fold 안에서 적용할 리샘플링 전략들\n",
        "resampling_strategies = [\n",
        "    \"none\",             # 리샘플링 없음 (원본 분포 그대로)\n",
        "    \"RandomOver\",\n",
        "    \"RandomUnder\",\n",
        "    \"SMOTE\",\n",
        "    \"SMOTEENN\",\n",
        "    \"SMOTETomek\",\n",
        "    \"ClusterCentroids\",\n",
        "]\n",
        "\n",
        "# -------------------- 타깃 0/1 표준화 --------------------\n",
        "def normalize_binary_target(y_raw: pd.Series) -> pd.Series:\n",
        "    y = y_raw.copy()\n",
        "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
        "    y_num = y_num.replace({7: np.nan, 9: np.nan})\n",
        "    uniq = set(pd.unique(y_num.dropna()))\n",
        "    if uniq.issubset({0, 1}):\n",
        "        return y_num.astype(\"Int64\")\n",
        "    if uniq.issubset({1, 2}):\n",
        "        return y_num.map({1: 1, 2: 0}).astype(\"Int64\")\n",
        "    # 문자열 대응\n",
        "    y_str = y.astype(str).str.strip().str.lower()\n",
        "    map_dict = {\"yes\": 1, \"y\": 1, \"true\": 1, \"1\": 1,\n",
        "                \"no\": 0, \"n\": 0, \"false\": 0, \"0\": 0}\n",
        "    y_map = y_str.map(map_dict)\n",
        "    return pd.to_numeric(y_map, errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# -------------------- 유틸 --------------------\n",
        "def detect_columns(X: pd.DataFrame):\n",
        "    obj_cats = list(X.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
        "    low_card = []\n",
        "    for c in X.columns:\n",
        "        if c in obj_cats:\n",
        "            continue\n",
        "        nunq = X[c].nunique(dropna=True)\n",
        "        if nunq <= 10:\n",
        "            low_card.append(c)\n",
        "    cat_cols = sorted(set(obj_cats + low_card))\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return cat_cols, num_cols\n",
        "\n",
        "def make_preprocessor(cat_cols, num_cols, scale_numeric=False):\n",
        "    try:\n",
        "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "\n",
        "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "    cat = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\",  ohe),\n",
        "    ])\n",
        "\n",
        "    if scale_numeric:\n",
        "        num = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scaler\",  StandardScaler()),\n",
        "        ])\n",
        "    else:\n",
        "        num = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        [(\"cat\", cat, cat_cols),\n",
        "         (\"num\", num, num_cols)],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "\n",
        "def make_models(y, use_class_weight=True):\n",
        "    try:\n",
        "        from xgboost import XGBClassifier as _XGB\n",
        "        XGB_OK = True\n",
        "    except ImportError:\n",
        "        XGB_OK = False\n",
        "        _XGB = None\n",
        "\n",
        "    cnt = Counter(y)\n",
        "    spw = (cnt.get(0, 0) / cnt.get(1, 1)) if cnt.get(1, 0) > 0 else 1.0\n",
        "    cw = \"balanced\" if use_class_weight else None\n",
        "\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            class_weight=cw,\n",
        "            n_jobs=-1,\n",
        "            random_state=seed\n",
        "        ),\n",
        "        \"MLP\": MLPClassifier(\n",
        "            hidden_layer_sizes=(64, 32),\n",
        "            activation=\"relu\",\n",
        "            alpha=1e-4,\n",
        "            learning_rate_init=1e-3,\n",
        "            max_iter=500,\n",
        "            early_stopping=True,\n",
        "            n_iter_no_change=10,\n",
        "            random_state=seed\n",
        "        ),\n",
        "        \"RandomForest\": RandomForestClassifier(\n",
        "            n_estimators=300,\n",
        "            class_weight=cw,\n",
        "            n_jobs=-1,\n",
        "            random_state=seed\n",
        "        ),\n",
        "        \"DecisionTree\": DecisionTreeClassifier(\n",
        "            class_weight=cw,\n",
        "            random_state=seed\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    if XGB_OK:\n",
        "        models[\"XGBoost\"] = _XGB(\n",
        "            n_estimators=500,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            reg_lambda=1.0,\n",
        "            objective=\"binary:logistic\",\n",
        "            eval_metric=\"logloss\",\n",
        "            tree_method=\"hist\",\n",
        "            n_jobs=-1,\n",
        "            random_state=seed,\n",
        "            scale_pos_weight=spw if use_class_weight else 1.0\n",
        "        )\n",
        "\n",
        "    return models\n",
        "\n",
        "def get_pos_scores(pipeline, X):\n",
        "    # 파이프라인 단위로 양성 클래스 점수 추출\n",
        "    if hasattr(pipeline, \"predict_proba\"):\n",
        "        try:\n",
        "            return pipeline.predict_proba(X)[:, 1]\n",
        "        except Exception:\n",
        "            pass\n",
        "    if hasattr(pipeline, \"decision_function\"):\n",
        "        s = pipeline.decision_function(X)\n",
        "        return s if s.ndim == 1 else s[:, 1]\n",
        "    return pipeline.predict(X)\n",
        "\n",
        "# 리샘플링 전략 이름 → imblearn 객체 매핑\n",
        "def make_resampler(name):\n",
        "    if name == \"none\":\n",
        "        return None\n",
        "    elif name == \"RandomOver\":\n",
        "        return RandomOverSampler(random_state=seed)\n",
        "    elif name == \"RandomUnder\":\n",
        "        return RandomUnderSampler(random_state=seed)\n",
        "    elif name == \"SMOTE\":\n",
        "        return SMOTE(random_state=seed)\n",
        "    elif name == \"SMOTEENN\":\n",
        "        return SMOTEENN(random_state=seed)\n",
        "    elif name == \"SMOTETomek\":\n",
        "        return SMOTETomek(random_state=seed)\n",
        "    elif name == \"ClusterCentroids\":\n",
        "        return ClusterCentroids(random_state=seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown resampling strategy: {name}\")\n",
        "\n",
        "# -------------------- 데이터 로드  --------------------\n",
        "if not os.path.exists(original_csv):\n",
        "    raise FileNotFoundError(f\"원본 CSV를 찾을 수 없습니다: {original_csv}\")\n",
        "\n",
        "df = pd.read_csv(original_csv)\n",
        "\n",
        "if target_col not in df.columns:\n",
        "    raise KeyError(f\"타깃 컬럼 '{target_col}' 이(가) 존재하지 않습니다.\")\n",
        "\n",
        "y_raw = df[target_col]\n",
        "y_bin = normalize_binary_target(y_raw)\n",
        "mask  = y_bin.notna()\n",
        "\n",
        "X = df.drop(columns=[target_col]).loc[mask].reset_index(drop=True)\n",
        "y = y_bin.loc[mask].astype(int).reset_index(drop=True)\n",
        "\n",
        "if not set(pd.unique(y)).issubset({0, 1}):\n",
        "    raise ValueError(\"타깃 라벨이 0/1 이외의 값을 포함하고 있습니다.\")\n",
        "\n",
        "cat_cols, num_cols = detect_columns(X)\n",
        "print(\"[INFO] 범주형 컬럼 수:\", len(cat_cols), \" / 수치형 컬럼 수:\", len(num_cols))\n",
        "\n",
        "# -------------------- CV 설정 --------------------\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "rows = []\n",
        "\n",
        "print(f\"[INFO] 리샘플링 전략: {len(resampling_strategies)}개, \"\n",
        "      f\"각 전략마다 모델×{cv.get_n_splits()}-fold 진행\")\n",
        "\n",
        "# -------------------- 리샘플링 × 모델 × CV --------------------\n",
        "for res_name in tqdm(resampling_strategies, desc=\"Resampling\"):\n",
        "    resampler = make_resampler(res_name)\n",
        "\n",
        "\n",
        "    ratio_pos = (y == 1).mean()\n",
        "    use_cw = (res_name == \"none\") and (ratio_pos < 0.3)\n",
        "    models = make_models(y, use_class_weight=use_cw)\n",
        "\n",
        "    for model_name, model in tqdm(models.items(), leave=False,\n",
        "                                  desc=f\"Models({res_name})\"):\n",
        "\n",
        "        pre = make_preprocessor(\n",
        "            cat_cols, num_cols,\n",
        "            scale_numeric=(model_name in [\"LogisticRegression\", \"MLP\"])\n",
        "        )\n",
        "\n",
        "        steps = [(\"preprocess\", pre)]\n",
        "        if resampler is not None:\n",
        "            steps.append((\"resample\", resampler))\n",
        "        steps.append((\"clf\", model))\n",
        "\n",
        "        pipe = ImbPipeline(steps)\n",
        "\n",
        "        accs, pres, recs, f1s, aucs = [], [], [], [], []\n",
        "\n",
        "        for tr_idx, te_idx in tqdm(cv.split(X, y),\n",
        "                                   total=cv.get_n_splits(),\n",
        "                                   leave=False,\n",
        "                                   desc=f\"{model_name} folds\"):\n",
        "            X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
        "            y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
        "\n",
        "            pipe.fit(X_tr, y_tr)\n",
        "\n",
        "            y_pred  = pipe.predict(X_te)\n",
        "            y_score = get_pos_scores(pipe, X_te)\n",
        "\n",
        "            accs.append(accuracy_score(y_te, y_pred))\n",
        "            pres.append(precision_score(y_te, y_pred,\n",
        "                                        average=\"macro\",\n",
        "                                        zero_division=0))\n",
        "            recs.append(recall_score(y_te, y_pred,\n",
        "                                     average=\"macro\",\n",
        "                                     zero_division=0))\n",
        "            f1s.append(f1_score(y_te, y_pred,\n",
        "                                average=\"macro\",\n",
        "                                zero_division=0))\n",
        "            try:\n",
        "                aucs.append(roc_auc_score(y_te, y_score))\n",
        "            except Exception:\n",
        "                aucs.append(np.nan)\n",
        "\n",
        "        rows.append({\n",
        "            \"resampling\": res_name,\n",
        "            \"model\": model_name,\n",
        "            \"accuracy\": f\"{np.mean(accs):.4f} ± {np.std(accs, ddof=1):.4f}\",\n",
        "            \"precision_macro\": f\"{np.mean(pres):.4f} ± {np.std(pres, ddof=1):.4f}\",\n",
        "            \"recall_macro\": f\"{np.mean(recs):.4f} ± {np.std(recs, ddof=1):.4f}\",\n",
        "            \"f1_macro\": f\"{np.mean(f1s):.4f} ± {np.std(f1s, ddof=1):.4f}\",\n",
        "            \"roc_auc\": f\"{np.nanmean(aucs):.4f} ± {np.nanstd(aucs, ddof=1):.4f}\",\n",
        "            \"_f1_mean\": np.mean(f1s),\n",
        "            \"_auc_mean\": np.nanmean(aucs)\n",
        "        })\n",
        "\n",
        "# -------------------- 결과 정리 & 저장 --------------------\n",
        "res_df = (pd.DataFrame(rows)\n",
        "          .sort_values(by=[\"resampling\", \"_f1_mean\", \"_auc_mean\"],\n",
        "                       ascending=[True, False, False]))\n",
        "\n",
        "display_cols = [\"resampling\", \"model\",\n",
        "                \"accuracy\", \"precision_macro\",\n",
        "                \"recall_macro\", \"f1_macro\", \"roc_auc\"]\n",
        "\n",
        "print(\"\\n[INFO] 교차검증 결과 요약:\")\n",
        "print(res_df[display_cols].to_string(index=False))\n",
        "\n",
        "out_csv = \"./cv_results_resampling_within_folds.csv\"\n",
        "res_df[display_cols].to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
        "print(f\"[INFO] 저장 완료 → {out_csv}\")\n"
      ],
      "metadata": {
        "id": "rxvw7FCRZ2sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn shap -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 1. 데이터 로드 + 타깃 0/1 표준화\n",
        "# ---------------------------------------------\n",
        "original_csv = \"/content/heart_disease_BRFSS2023.csv\"\n",
        "target_col   = \"_MICHD\"\n",
        "\n",
        "df = pd.read_csv(original_csv)\n",
        "\n",
        "def normalize_binary_target(y_raw: pd.Series) -> pd.Series:\n",
        "    y = y_raw.copy()\n",
        "    y_num = pd.to_numeric(y, errors=\"coerce\")\n",
        "    y_num = y_num.replace({7: np.nan, 9: np.nan})\n",
        "    uniq = set(pd.unique(y_num.dropna()))\n",
        "    if uniq.issubset({0, 1}):\n",
        "        return y_num.astype(\"Int64\")\n",
        "    if uniq.issubset({1, 2}):\n",
        "        return y_num.map({1: 1, 2: 0}).astype(\"Int64\")\n",
        "    # 문자열 대응\n",
        "    y_str = y.astype(str).str.strip().str.lower()\n",
        "    map_dict = {\"yes\": 1, \"y\": 1, \"true\": 1, \"1\": 1,\n",
        "                \"no\": 0, \"n\": 0, \"false\": 0, \"0\": 0}\n",
        "    y_map = y_str.map(map_dict)\n",
        "    return pd.to_numeric(y_map, errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "y_raw = df[target_col]\n",
        "y_bin = normalize_binary_target(y_raw)\n",
        "mask  = y_bin.notna()\n",
        "\n",
        "X = df.drop(columns=[target_col]).loc[mask].reset_index(drop=True)\n",
        "y = y_bin.loc[mask].astype(int).reset_index(drop=True)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 2. 범주형/수치형 컬럼 구분 + 전처리 정의\n",
        "# ---------------------------------------------\n",
        "def detect_columns(X: pd.DataFrame):\n",
        "    obj_cats = list(X.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
        "    low_card = []\n",
        "    for c in X.columns:\n",
        "        if c in obj_cats:\n",
        "            continue\n",
        "        nunq = X[c].nunique(dropna=True)\n",
        "        if nunq <= 10:\n",
        "            low_card.append(c)\n",
        "    cat_cols = sorted(set(obj_cats + low_card))\n",
        "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "    return cat_cols, num_cols\n",
        "\n",
        "cat_cols, num_cols = detect_columns(X)\n",
        "\n",
        "def make_preprocessor(cat_cols, num_cols, scale_numeric=True):\n",
        "    # sklearn 버전별 대응\n",
        "    try:\n",
        "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\",  ohe),\n",
        "    ])\n",
        "\n",
        "    if scale_numeric:\n",
        "        num_pipe = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scaler\",  StandardScaler()),\n",
        "        ])\n",
        "    else:\n",
        "        num_pipe = Pipeline([\n",
        "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        [(\"cat\", cat_pipe, cat_cols),\n",
        "         (\"num\", num_pipe, num_cols)],\n",
        "        remainder=\"drop\"\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "preprocessor = make_preprocessor(cat_cols, num_cols, scale_numeric=True)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 3. Train/Test 분할\n",
        "# ---------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=seed\n",
        ")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 4. 리샘플링 & 모델 정의\n",
        "# ---------------------------------------------\n",
        "\n",
        "#  [리샘플링 방법 교체 포인트]\n",
        "#   - 현재: SMOTETomek\n",
        "#   - 예시:\n",
        "#       * 리샘플링 안 하고 싶을 때:\n",
        "#           → 아래 resampler 관련 스텝을 ImbPipeline에서 제거\n",
        "#           resampler = RandomUnderSampler(random_state=seed)\n",
        "#           resampler = RandomOverSampler(random_state=seed)\n",
        "#           resampler = SMOTE(random_state=seed)\n",
        "#           resampler = SMOTEENN(random_state=seed)\n",
        "#           resampler = ClusterCentroids(random_state=seed)\n",
        "resampler = SMOTETomek(random_state=seed)\n",
        "\n",
        "# [모델 교체 포인트]\n",
        "#   - 현재: XGBClassifier\n",
        "#   - 예시:\n",
        "#           clf = RandomForestClassifier(\n",
        "#               n_estimators=300,\n",
        "#               max_depth=None,\n",
        "#               n_jobs=-1,\n",
        "#               random_state=seed\n",
        "#           )\n",
        "#           clf = LogisticRegression(\n",
        "#               max_iter=2000,\n",
        "#               n_jobs=-1,\n",
        "#               random_state=seed\n",
        "#           )\n",
        "#           clf = MLPClassifier(\n",
        "#               hidden_layer_sizes=(64, 32),\n",
        "#               max_iter=300,\n",
        "#               random_state=seed\n",
        "#           )\n",
        "#           clf = DecisionTreeClassifier(\n",
        "#               max_depth=6,\n",
        "#               random_state=seed\n",
        "#           )\n",
        "#\n",
        "\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=-1,\n",
        "    random_state=seed,\n",
        "    scale_pos_weight=1.0,\n",
        ")\n",
        "\n",
        "clf = xgb  # ← 위에서 다른 모델로 교체했다면 여기 교체\n",
        "\n",
        "# ImbPipeline 정의\n",
        "pipe = ImbPipeline([\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"resample\",  resampler),  # 리샘플링 끄려면 이 줄을 제거\n",
        "    (\"clf\",       clf),\n",
        "])\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 5. SHAP을 위한 입력 준비\n",
        "# ---------------------------------------------\n",
        "\n",
        "X_test_proc = pipe.named_steps[\"preprocess\"].transform(X_test)\n",
        "feature_names = pipe.named_steps[\"preprocess\"].get_feature_names_out()\n",
        "\n",
        "xgb_model = pipe.named_steps[\"clf\"]\n",
        "\n",
        "\n",
        "explainer = shap.TreeExplainer(xgb_model)\n",
        "\n",
        ")\n",
        "shap_values = explainer.shap_values(X_test_proc, check_additivity=False)\n",
        "\n",
        "\n",
        "if isinstance(shap_values, list):\n",
        "\n",
        "    shap_to_plot = shap_values[1]\n",
        "else:\n",
        "    shap_to_plot = shap_values\n",
        "\n",
        "# 6) summary_plot에 쓸 DataFrame (열 개수를 SHAP 값에 딱 맞게)\n",
        "n_features = shap_to_plot.shape[1]\n",
        "X_test_for_plot = pd.DataFrame(\n",
        "    X_test_proc[:, :n_features],\n",
        "    columns=feature_names[:n_features]\n",
        ")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# 7. Summary plot (beeswarm + bar)\n",
        "# ---------------------------------------------\n",
        "plt.figure(figsize=(6, 8))\n",
        "shap.summary_plot(shap_to_plot, X_test_for_plot, show=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 8))\n",
        "shap.summary_plot(shap_to_plot, X_test_for_plot, plot_type=\"bar\", show=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-YRcDY2bXrDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xt3iCh3RaOEw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}